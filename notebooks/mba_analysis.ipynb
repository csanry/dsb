{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib as mpl \n",
    "# import missingno as msno\n",
    "import seaborn as sns\n",
    "\n",
    "from src import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 112650 entries, 0 to 112649\n",
      "Data columns (total 7 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   order_id             112650 non-null  object \n",
      " 1   order_item_id        112650 non-null  int64  \n",
      " 2   product_id           112650 non-null  object \n",
      " 3   seller_id            112650 non-null  object \n",
      " 4   shipping_limit_date  112650 non-null  object \n",
      " 5   price                112650 non-null  float64\n",
      " 6   freight_value        112650 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 6.0+ MB\n",
      "order_id               0\n",
      "order_item_id          0\n",
      "product_id             0\n",
      "seller_id              0\n",
      "shipping_limit_date    0\n",
      "price                  0\n",
      "freight_value          0\n",
      "dtype: int64\n",
      "\n",
      "None\n",
      "None\n",
      "(112650, 7)\n",
      "                           order_id  order_item_id  \\\n",
      "0  00010242fe8c5a6d1ba2dd792cb16214              1   \n",
      "1  00018f77f2f0320c557190d7a144bdd3              1   \n",
      "2  000229ec398224ef6ca0657da4fc703e              1   \n",
      "3  00024acbcdf0a6daa1e931b038114c75              1   \n",
      "4  00042b26cf59d7ce69dfabb4e55b4fd9              1   \n",
      "\n",
      "                         product_id                         seller_id  \\\n",
      "0  4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
      "1  e5f2d52b802189ee658865ca93d83a8f  dd7ddc04e1b6c2c614352b383efe2d36   \n",
      "2  c777355d18b72b67abbeef9df44fd0fd  5b51032eddd242adc84c38acab88f23d   \n",
      "3  7634da152a4610f1595efa32f14722fc  9d7a1d34a5052409006425275ba1c2b4   \n",
      "4  ac6c3623068f30de03045865e4e10089  df560393f3a51e74553ab94004ba5c87   \n",
      "\n",
      "   shipping_limit_date   price  freight_value  \n",
      "0  2017-09-19 09:45:35   58.90          13.29  \n",
      "1  2017-05-03 11:05:13  239.90          19.93  \n",
      "2  2018-01-18 14:48:30  199.00          17.87  \n",
      "3  2018-08-15 10:10:18   12.99          12.79  \n",
      "4  2017-02-13 13:57:51  199.90          18.14  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(config.RAW_FILE_PATH / 'olist_order_items_dataset.csv')\n",
    "\n",
    "print(f'''\n",
    "{df.info()}\n",
    "{print(df.isna().sum())}\n",
    "{df.shape}\n",
    "{df.head()}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('order_id').agg({'product_id': lambda x: list(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.4 MB 122 kB/s eta 0:00:011   |█▏                              | 10.3 MB 3.8 MB/s eta 0:01:12     |█████████▏                      | 80.4 MB 231 kB/s eta 0:14:27     |███████████▍                    | 100.3 MB 9.4 MB/s eta 0:00:20     |████████████████▎               | 143.4 MB 15.6 MB/s eta 0:00:09     |████████████████▊               | 146.7 MB 15.6 MB/s eta 0:00:09     |█████████████████               | 149.2 MB 15.3 MB/s eta 0:00:09     |█████████████████▏              | 151.1 MB 15.3 MB/s eta 0:00:09     |██████████████████▏             | 160.2 MB 3.7 MB/s eta 0:00:34     |██████████████████▎             | 160.6 MB 3.7 MB/s eta 0:00:34     |██████████████████▌             | 162.8 MB 3.7 MB/s eta 0:00:33     |███████████████████▎            | 169.7 MB 8.1 MB/s eta 0:00:14     |████████████████████▍           | 179.0 MB 12.8 MB/s eta 0:00:08     |███████████████████████▍        | 205.6 MB 13.6 MB/s eta 0:00:06     |████████████████████████▎       | 213.6 MB 9.5 MB/s eta 0:00:08     |████████████████████████▉       | 218.0 MB 15.4 MB/s eta 0:00:05     |█████████████████████████▏      | 221.8 MB 15.4 MB/s eta 0:00:04     |████████████████████████████▋   | 251.9 MB 17.2 MB/s eta 0:00:02     |█████████████████████████████   | 255.8 MB 17.2 MB/s eta 0:00:02     |█████████████████████████████▉  | 262.5 MB 18.6 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=039c68c25827ef0da0854b215a0dacb261d7dfa54046cb0048555ca5b0eb1e67\n",
      "  Stored in directory: /root/.cache/pip/wheels/52/45/50/69db7b6e1da74a1b9fcc097827db9185cb8627117de852731e\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://archive.ubuntu.com/ubuntu bionic InRelease                        \n",
      "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]    \n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]      \n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]    \n",
      "Get:5 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1512 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [991 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3231 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2286 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [957 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2799 kB]\n",
      "Fetched 12.0 MB in 23s (533 kB/s)                                              \n",
      "Reading package lists... Done\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n"
     ]
    }
   ],
   "source": [
    "!apt-get update --fix-missing\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T \n",
    "import pyspark.sql.functions as F \n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/06 04:06:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conf = SparkConf().setAppName(\"base\").setMaster(\"local[*]\")\n",
    "# spark = SparkContext(conf=conf).getOrCreate()\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date| price|freight_value|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.90|        13.29|\n",
      "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.90|        19.93|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.options(header='true').csv('../data/raw/olist_order_items_dataset.csv')\n",
    "\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date| price|freight_value|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.90|        13.29|\n",
      "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.90|        19.93|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates(['order_id', 'product_id']).sort('order_id')\n",
    "\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+\n",
      "|            order_id|collect_list(product_id)|\n",
      "+--------------------+------------------------+\n",
      "|00010242fe8c5a6d1...|    [4244733e06e7ecb4...|\n",
      "|00018f77f2f0320c5...|    [e5f2d52b802189ee...|\n",
      "+--------------------+------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.groupBy('order_id').agg(F.collect_list('product_id')).sort('order_id')\n",
    "\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fpGrowth = FPGrowth(itemsCol='collect_list(product_id)', minSupport=0.00001, minConfidence=0.0001)\n",
    "\n",
    "model = fpGrowth.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dsb/lib/python3.9/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "22/06/06 04:07:07 WARN DAGScheduler: Broadcasting large task binary with size 1495.3 KiB\n",
      "22/06/06 04:07:08 WARN DAGScheduler: Broadcasting large task binary with size 1150.4 KiB\n",
      "22/06/06 04:07:08 WARN DAGScheduler: Broadcasting large task binary with size 1149.8 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|               items|freq|\n",
      "+--------------------+----+\n",
      "|[ec58535d820b93ed...|   1|\n",
      "|[be7f0912ba011267...|   5|\n",
      "|[868766ed2172644f...|   1|\n",
      "|[a94bae2ca6c0f653...|   3|\n",
      "|[ceea7d6ebaeb5a36...|   1|\n",
      "|[0bb1142b8037490f...|   1|\n",
      "|[54df5de144ebb216...|   1|\n",
      "|[75d3f426c010ad14...|   1|\n",
      "|[3df78dbb35e3edc3...|   1|\n",
      "|[f4470c119e77b798...|   1|\n",
      "|[cb7433cfec8ba4a5...|   2|\n",
      "|[71e5d0eb5a5fcc82...|   5|\n",
      "|[ff4c1a248a5110d7...|   2|\n",
      "|[0d0bc6cf9765ffe8...|   3|\n",
      "|[ca4ac4e6aa504eee...|   1|\n",
      "|[9c711b24a6ec15a6...|   1|\n",
      "|[e449c89deefdb41b...|   1|\n",
      "|[5e7cc48697a854bb...|   1|\n",
      "|[55ba658d5c2f0b1f...|   1|\n",
      "|[0e8c1dc0559f4a9a...|   1|\n",
      "+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dsb/lib/python3.9/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "22/06/06 04:07:09 WARN DAGScheduler: Broadcasting large task binary with size 1155.3 KiB\n",
      "22/06/06 04:07:09 WARN DAGScheduler: Broadcasting large task binary with size 1155.3 KiB\n",
      "22/06/06 04:07:10 WARN DAGScheduler: Broadcasting large task binary with size 1605.8 KiB\n",
      "22/06/06 04:07:10 WARN DAGScheduler: Broadcasting large task binary with size 1604.5 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+------------------+--------------------+\n",
      "|          antecedent|          consequent|         confidence|              lift|             support|\n",
      "+--------------------+--------------------+-------------------+------------------+--------------------+\n",
      "|[e65ab426efaf65e0...|[4d5bb93bfa70f67c...|                1.0|           49333.0|1.013520361624065...|\n",
      "|[a5d89252158ff2fc...|[aaac7cb8bbb8fe12...|                1.0|           98666.0|1.013520361624065...|\n",
      "|[50046ff3c7e88683...|[ac1fcaf7402bfbf0...|                1.0|           98666.0|1.013520361624065...|\n",
      "|[50046ff3c7e88683...|[d5bf1e6386a450dd...|                1.0|           98666.0|1.013520361624065...|\n",
      "|[50046ff3c7e88683...|[9eacfe990da87a66...|                1.0|           98666.0|1.013520361624065...|\n",
      "|[ef7de0bbb0e0b011...|[7da9810448a8f92f...| 0.3333333333333333|16444.333333333332|1.013520361624065...|\n",
      "|[5b8a5a9417210b1b...|[fe59a1e006df3ac4...|0.16666666666666666|1494.9393939393938|1.013520361624065...|\n",
      "|[a12fc4a1c79238df...|[aa36c2127048efdc...|                1.0| 8969.636363636364|1.013520361624065...|\n",
      "|[a12fc4a1c79238df...|[724692d1ae89a580...|                1.0|           49333.0|1.013520361624065...|\n",
      "|[f52b6b9e0bc35ec9...|[c8078bb7927143cd...|                1.0|32888.666666666664|1.013520361624065...|\n",
      "|[f52b6b9e0bc35ec9...|[3ffc8b6c3ae2afbf...|                1.0|           24666.5|1.013520361624065...|\n",
      "|[6a1495163425aad8...|[0d944ade388f4db6...|                1.0|           98666.0|1.013520361624065...|\n",
      "|[c804537dd3a8ff5f...|[eb048e296b8db6cd...|                1.0|           49333.0|1.013520361624065...|\n",
      "|[c804537dd3a8ff5f...|[a1dd59756b92c62c...|                1.0|10962.888888888889|1.013520361624065...|\n",
      "|[40b6762970c412b5...|[c64fe38b4cd06cf7...|                1.0|           98666.0|1.013520361624065...|\n",
      "|[cb61a9727a2f74ec...|[a237de12bdf0bfe4...| 0.3333333333333333|1096.2888888888888|1.013520361624065...|\n",
      "|[b520bae2a8c9790c...|[0dc2c7b7e9708ca6...|0.16666666666666666|16444.333333333332|1.013520361624065...|\n",
      "|[fbaefac04950112a...|[c90526921508388b...| 0.3333333333333333|32888.666666666664|1.013520361624065...|\n",
      "|[fbaefac04950112a...|[e4da15679c5e9692...| 0.3333333333333333|32888.666666666664|1.013520361624065...|\n",
      "|[0a4093a4af429dc0...|[ede062bcd13ef64b...|              0.025|246.66500000000002|1.013520361624065...|\n",
      "+--------------------+--------------------+-------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.freqItemsets.show()\n",
    "items = model.freqItemsets.toPandas()\n",
    "\n",
    "model.associationRules.show()\n",
    "rules = model.associationRules.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.to_csv('./mba_freq_items.csv')\n",
    "rules.to_csv('./mba_pattern.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "44a3c08237e01b3f273f94d4b0b6f4bfc3fd3da5036836380d570cd78e561537"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
